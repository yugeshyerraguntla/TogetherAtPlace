# TogetherAtPlace
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Python Basics ## 
-- [BASICS OF PYTHON](https://github.com/yugeshyerraguntla/PythonBasics)

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Basic ML Together at a place
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Link for all repositories

Data ---> Feature Engineering ---> Handle Categorical Features ---> Feature Selection ---> ML Algos ---> HyperParameter Tuning ---> Deployment


1. Feature Engineering        -- [FEATURE ENGINEERING TECHNIQUES](https://github.com/yugeshyerraguntla/FeatureEngineeringTechniques)
    - Handling Missing Data - (Mean/MedianMode Replacement, Random Sample Imputation, Capturing NaN values, EoD Imputation, Arbitrary Imputation)
    - Handling Categorical Variables - (Highest Count, (Nominal and Ordinal Variables)-One Hot Encoding, Count Frequency Encoding)
    - Feature Scaling - (Normalization, Standardization)


2. Feature Selection          -- [FEATURE SELECTION TECHNIQUES](https://github.com/yugeshyerraguntla/FeatureSelectionTechniques)
    - Dropping Constan Features - (Variance Threshold)
    - Correlation 
    - Information Gain - Mutual Information Gain
    - Chi Square - Fisher Score (For Feature Selection)
    - KBestAlgorithm


3. Handling Imbalanced Data   -- [HANDLING IMBALANCED DATA](https://github.com/yugeshyerraguntla/HandlingImbalancedData)
    - Try reducing FP and FN. Look at your Performance Metrics and apply SMOTE TECHNIQUES
    - If nothing works, go for Ensmble techniques


4. Hypothesis Testing         -- [HYPOTHESIS TESTING](https://github.com/yugeshyerraguntla/HypothesisTesting---T-ChiSquare-Anova)
    - Z Test                    -   Used when the population mean and std deviation are known
    - One Sample Prop Test      -   For One Category Variable
    - ChiSquare Test            -   For Two Category Features (Two sample Prop Test)
    - T Test                    -   One Continuous Feature is taken. (Also used when Std.Deviation is unknown)
    - Correlation               -   Two Continuous Features are taken (Also we can use Two Sample T Test)
    - ANOVA Test                -   One Numerical Var + One Categorical Var / Categorical Var with more than 3 features)

    As per Prof.UDK, THe Objective of various tests are as follows:
     - Hypothesis Test   - To Reject or Retain existing claim or belief.
     - Z Test            - To test the value of population mean when the population variance is known
     - T Test(one sample)- To test when the Std Deviation is unknown and estimated from sample
     - T Test(two sample)- To test difference btw toe population means where Std Deviations are unknown
     - PairedSampleTTest - To test whether an Intervention may have significantly changed the population parameters
     - Chi Squared Test  - Non Parameteric test for caomparing Observed Dist Data vs Expected Dist Data
     - ANOVAV            - To Compare Mean values simultaneously for more than 2 groups
           

5. Types of Transformation    --  [Types of Transformation](https://github.com/yugeshyerraguntla/TypesOfTransformation)

6. Types of Cross Validation  --  [Types Of Cross Validation](https://github.com/yugeshyerraguntla/CrossValidationTypes)

7. Hyperprameter Optimization --  [HyperParameter Optimization](https://github.com/yugeshyerraguntla/HyperParameterOptimization)



**Machine Learning Algorithms:**

- [All Basic ML Models](https://github.com/yugeshyerraguntla/Basic-MachineLearning-Models)
  - Linear, Logistic, RandomForest, KNN, KMeans, NLP, SVM


- [Linear Regression](https://github.com/yugeshyerraguntla/Linear-Regression-Simple-Ridge-Lasso-Multiple-Polynomial)
  - Includes Simple Linear Reg, Ridge Reg, Lasso Reg, Multiple Reg, Polynomial Reg.


- [Decision Trees](https://github.com/yugeshyerraguntla/DecisionTrees)


- [Logistic Regression](https://github.com/yugeshyerraguntla/LogisticRegression)


- [Naive Bayes](https://github.com/yugeshyerraguntla/NaiveBayes)


- [KNearestNeighbours](https://github.com/yugeshyerraguntla/KNearestNeighbours)


- [CLUSTERING - KMeans, DBSCAN](https://github.com/yugeshyerraguntla/Clustering-KMeans-DBSCAN)


- [ENSEMBLE TECHNIQUES](https://github.com/yugeshyerraguntla/EnsembleLearning---Comparing-Various-Models)

  - [ENSEMBLE - Bagging](https://github.com/yugeshyerraguntla/Ensemble-Bagging)
    - Bootstrap Aggregation - Randomforest

  - [ENSEMBLE - Boosting](https://github.com/yugeshyerraguntla/Ensemble-Boosting)
    -  ADA Boost
    -  XG Boost




**Sample ML Projects:**

- [500+ AI-ML Projects](https://github.com/yugeshyerraguntla/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code)

- [2019 WorldCup Winner Predictor with RF](https://github.com/yugeshyerraguntla/2019-ICC-WorldCupPredictor_RF)

- [Drowsiness Detection](https://github.com/yugeshyerraguntla/DrowsinessDetection)








